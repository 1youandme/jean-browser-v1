{'role': 'assistant', 'content': '\n<function_calls>\n<create-file file_path="src-tauri/src/ai.rs">\nuse axum::{Json, extract::State};\nuse serde::{Deserialize, Serialize};\nuse serde_json::Value;\nuse std::collections::HashMap;\n\n#[derive(Debug, Deserialize)]\npub struct AIRequest {\n    prompt: String,\n    context_json: Option<String>,\n    model: Option<String>,\n}\n\n#[derive(Debug, Serialize)]\npub struct AIResponse {\n    response: String,\n    actions: Option<Vec<JeanAction>>,\n}\n\n#[derive(Debug, Serialize, Deserialize)]\n#[serde(tag = "type")]\npub enum JeanAction {\n    OpenTab {\n        target_bar: String,\n        payload: serde_json::Value,\n    },\n    CloseTab {\n        target_bar: String,\n        payload: serde_json::Value,\n    },\n    Navigate {\n        target_bar: String,\n        payload: serde_json::Value,\n    },\n    OpenLocalPath {\n        target_bar: String,\n        payload: serde_json::Value,\n    },\n}\n\npub async fn generate_response(\n    Json(request): Json<AIRequest>,\n) -> Result<Json<AIResponse>, axum::http::StatusCode> {\n    // Check if we should use cloud AI or local LLM\n    let use_cloud_ai = std::env::var("USE_CLOUD_AI").unwrap_or_else(|_| "false".to_string()) == "true";\n    \n    let response = if use_cloud_ai {\n        call_cloud_llm(&request).await\n    } else {\n        call_local_llm(&request).await\n    };\n\n    match response {\n        Ok(ai_response) => Ok(Json(ai_response)),\n        Err(e) => {\n            tracing::error!("AI generation failed: {}", e);\n            Err(axum::http::StatusCode::INTERNAL_SERVER_ERROR)\n        }\n    }\n}\n\nasync fn call_cloud_llm(request: &AIRequest) -> Result<AIResponse, Box<dyn std::error::Error>> {\n    let cloud_endpoint = std::env::var("CLOUD_AI_ENDPOINT")?;\n    let api_key = std::env::var("CLOUD_AI_API_KEY").ok();\n\n    let client = reqwest::Client::new();\n    let mut req_body = HashMap::new();\n    req_body.insert("prompt", &request.prompt);\n    if let Some(context) = &request.context_json {\n        req_body.insert("context", context);\n    }\n    if let Some(model) = &request.model {\n        req_body.insert("model", model);\n    }\n\n    let mut http_req = client.post(&cloud_endpoint).json(&req_body);\n    if let Some(key) = api_key {\n        http_req = http_req.header("Authorization", format!("Bearer {}", key));\n    }\n\n    let response: Value = http_req.send().await?.json().await?;\n    \n    // Parse response and extract actions\n    let response_text = response["response"].as_str().unwrap_or("I\'m sorry, I couldn\'t process that request.");\n    let actions = extract_actions_from_response(&response);\n\n    Ok(AIResponse {\n        response: response_text.to_string(),\n        actions: Some(actions),\n    })\n}\n\nasync fn call_local_llm(request: &AIRequest) -> Result<AIResponse, Box<dyn std::error::Error>> {\n    let local_endpoint = std::env::var("LOCAL_LLM_ENDPOINT")\n        .unwrap_or_else(|_| "http://localhost:8081/v1/chat/completions".to_string());\n\n    let client = reqwest::Client::new();\n    let req_body = serde_json::json!({\n        "messages": [\n            {\n                "role": "system",\n                "content": "You are Jean, an AI assistant for the JeanTrail Browser. Help users navigate and manage their browsing experience. When appropriate, suggest actions in JSON format."\n            },\n            {\n                "role": "user", \n                "content": format!("{}\\n\\nContext: {}", request.prompt, request.context_json.as_deref().unwrap_or("None"))\n            }\n        ],\n        "model": request.model.as_deref().unwrap_or("default"),\n        "max_tokens": 1000\n    });\n\n    let response: Value = client.post(&local_endpoint).json(&req_body).send().await?.json().await?;\n    \n    let response_text = response["choices"][0]["message"]["content"]\n        .as_str()\n        .unwrap_or("I\'m sorry, I couldn\'t process that request.");\n\n    let actions = extract_actions_from_response(&response);\n\n    Ok(AIResponse {\n        response: response_text.to_string(),\n        actions: Some(actions),\n    })\n}\n\nfn extract_actions_from_response(response: &Value) -> Vec<JeanAction> {\n    // Look for JSON actions in the response text\n    // For now, return empty - this would be enhanced to parse actual actions\n    Vec::new()\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[tokio::test]\n    async fn test_ai_response_parsing() {\n        let request = AIRequest {\n            prompt: "Open a new tab".to_string(),\n            context_json: None,\n            model: None,\n        };\n\n        // Test would go here\n    }\n}\n</create-file>\n</function_calls>'}