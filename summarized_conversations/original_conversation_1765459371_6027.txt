{'role': 'assistant', 'content': '\n## üéØ Creating Enhanced Product Scraper\n\nNow I\'ll create the enhanced product scraper with all the requested features:\n\n<function_calls>\n<create-file file_path="jeantrail_project/enhanced_scraper.py">\n#!/usr/bin/env python3\n"""\nEnhanced Product Scraper for JeanTrail Browser\nAdvanced web scraping with multi-account support, error handling, and data extraction\n"""\n\nimport asyncio\nimport aiohttp\nimport json\nimport logging\nimport random\nimport time\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom dataclasses import dataclass, asdict\nfrom urllib.parse import urljoin, urlparse\nimport re\nimport hashlib\nimport base64\nfrom pathlib import Path\nimport sqlite3\nfrom contextlib import asynccontextmanager\nimport csv\nimport os\nimport sys\nfrom enum import Enum\n\n# Third-party imports\ntry:\n    from bs4 import BeautifulSoup\n    from selenium import webdriver\n    from selenium.webdriver.chrome.options import Options\n    from selenium.webdriver.common.by import By\n    from selenium.webdriver.support.ui import WebDriverWait\n    from selenium.webdriver.support import expected_conditions as EC\n    from selenium.common.exceptions import TimeoutException, WebDriverException\n    import undetected_chromedriver as uc\n    import fake_useragent\n    import requests\n    from email.mime.multipart import MIMEMultipart\n    from email.mime.text import MIMEText\n    import smtplib\n    from concurrent.futures import ThreadPoolExecutor, as_completed\n    import pandas as pd\n    import numpy as np\n    from google.auth.transport.requests import Request\n    from google.oauth2.credentials import Credentials\n    from google_auth_oauthlib.flow import InstalledAppFlow\n    from googleapiclient.discovery import build\n    from googleapiclient.errors import HttpError\nexcept ImportError as e:\n    print(f"Missing required dependency: {e}")\n    print("Please install with: pip install beautifulsoup4 selenium undetected-chromedriver fake-useragent requests pandas numpy google-api-python-client google-auth-httplib2 google-auth-oauthlib")\n    sys.exit(1)\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\',\n    handlers=[\n        logging.FileHandler(\'enhanced_scraper.log\'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\nclass ScraperStatus(Enum):\n    IDLE = "idle"\n    RUNNING = "running"\n    PAUSED = "paused"\n    ERROR = "error"\n    COMPLETED = "completed"\n\n@dataclass\nclass ProductData:\n    """Enhanced product data structure"""\n    id: str\n    title: str\n    description: str\n    price: float\n    currency: str\n    availability: str\n    rating: Optional[float]\n    reviews_count: Optional[int]\n    images: List[str]\n    categories: List[str]\n    tags: List[str]\n    seller: str\n    seller_url: Optional[str]\n    upload_date: Optional[datetime]\n    last_updated: datetime\n    url: str\n    source: str\n    specifications: Dict[str, Any]\n    shipping_info: Dict[str, Any]\n    variant_info: Dict[str, Any]\n    metadata: Dict[str, Any]\n\n@dataclass\nclass ScraperConfig:\n    """Scraper configuration"""\n    max_retries: int = 3\n    retry_delay: float = 2.0\n    request_timeout: int = 30\n    concurrent_requests: int = 5\n    delay_between_requests: float = 1.0\n    use_proxies: bool = True\n    use_selenium: bool = False\n    headless: bool = True\n    max_products_per_run: int = 100\n    enable_categorization: bool = True\n    save_images: bool = False\n    enable_notifications: bool = True\n\n@dataclass\nclass ProxyConfig:\n    """Proxy configuration"""\n    enabled: bool = False\n    proxy_list: List[str] = None\n    rotation_enabled: bool = True\n    current_proxy_index: int = 0\n\n@dataclass\nclass GmailConfig:\n    """Gmail configuration for multiple accounts"""\n    credentials_file: str = \'credentials.json\'\n    token_file: str = \'token.json\'\n    accounts: List[Dict[str, str]] = None\n    current_account_index: int = 0\n\nclass ProxyManager:\n    """Proxy rotation and management"""\n    \n    def __init__(self, proxy_config: ProxyConfig):\n        self.config = proxy_config\n        self.failed_proxies = set()\n        \n    def get_proxy(self) -> Optional[str]:\n        """Get next available proxy"""\n        if not self.config.enabled or not self.config.proxy_list:\n            return None\n            \n        available_proxies = [\n            proxy for proxy in self.config.proxy_list \n            if proxy not in self.failed_proxies\n        ]\n        \n        if not available_proxies:\n            logger.warning("No available proxies")\n            return None\n            \n        if self.config.rotation_enabled:\n            proxy = available_proxies[self.config.current_proxy_index % len(available_proxies)]\n            self.config.current_proxy_index += 1\n        else:\n            proxy = random.choice(available_proxies)\n            \n        return proxy\n    \n    def mark_proxy_failed(self, proxy: str):\n        """Mark proxy as failed"""\n        self.failed_proxies.add(proxy)\n        logger.warning(f"Proxy marked as failed: {proxy}")\n    \n    def reset_failed_proxies(self):\n        """Reset failed proxy list"""\n        self.failed_proxies.clear()\n\nclass DatabaseManager:\n    """SQLite database management for products and scraping sessions"""\n    \n    def __init__(self, db_path: str = "products.db"):\n        self.db_path = db_path\n        self.init_database()\n    \n    def init_database(self):\n        """Initialize database tables"""\n        with sqlite3.connect(self.db_path) as conn:\n            conn.execute("""\n                CREATE TABLE IF NOT EXISTS products (\n                    id TEXT PRIMARY KEY,\n                    title TEXT NOT NULL,\n                    description TEXT,\n                    price REAL,\n                    currency TEXT,\n                    availability TEXT,\n                    rating REAL,\n                    reviews_count INTEGER,\n                    images TEXT,  -- JSON array\n                    categories TEXT,  -- JSON array\n                    tags TEXT,  -- JSON array\n                    seller TEXT,\n                    seller_url TEXT,\n                    upload_date TEXT,\n                    last_updated TEXT,\n                    url TEXT NOT NULL,\n                    source TEXT NOT NULL,\n                    specifications TEXT,  -- JSON object\n                    shipping_info TEXT,  -- JSON object\n                    variant_info TEXT,  -- JSON object\n                    metadata TEXT  -- JSON object\n                )\n            """)\n            \n            conn.execute("""\n                CREATE TABLE IF NOT EXISTS scraping_sessions (\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n                    start_time TEXT,\n                    end_time TEXT,\n                    status TEXT,\n                    products_scraped INTEGER,\n                    errors_count INTEGER,\n                    source TEXT,\n                    config TEXT  -- JSON object\n                )\n            """)\n            \n            conn.execute("""\n                CREATE TABLE IF NOT EXISTS scraping_errors (\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n                    session_id INTEGER,\n                    url TEXT,\n                    error_type TEXT,\n                    error_message TEXT,\n                    timestamp TEXT,\n                    FOREIGN KEY (session_id) REFERENCES scraping_sessions (id)\n                )\n            """)\n            \n            conn.commit()\n    \n    def save_product(self, product: ProductData) -> bool:\n        """Save product to database"""\n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                conn.execute("""\n                    INSERT OR REPLACE INTO products \n                    (id, title, description, price, currency, availability, rating, \n                     reviews_count, images, categories, tags, seller, seller_url, \n                     upload_date, last_updated, url, source, specifications, \n                     shipping_info, variant_info, metadata)\n                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n                """, (\n                    product.id,\n                    product.title,\n                    product.description,\n                    product.price,\n                    product.currency,\n                    product.availability,\n                    product.rating,\n                    product.reviews_count,\n                    json.dumps(product.images),\n                    json.dumps(product.categories),\n                    json.dumps(product.tags),\n                    product.seller,\n                    product.seller_url,\n                    product.upload_date.isoformat() if product.upload_date else None,\n                    product.last_updated.isoformat(),\n                    product.url,\n                    product.source,\n                    json.dumps(product.specifications),\n                    json.dumps(product.shipping_info),\n                    json.dumps(product.variant_info),\n                    json.dumps(product.metadata)\n                ))\n                conn.commit()\n            return True\n        except Exception as e:\n            logger.error(f"Failed to save product: {e}")\n            return False\n    \n    def get_products(self, limit: Optional[int] = None, source: Optional[str] = None) -> List[ProductData]:\n        """Get products from database"""\n        try:\n            with sqlite3.connect(self.db_path) as conn:\n                query = "SELECT * FROM products"\n                params = []\n                \n                if source:\n                    query += " WHERE source = ?"\n                    params.append(source)\n                \n                if limit:\n                    query += " LIMIT ?"\n                    params.append(limit)\n                \n                cursor = conn.execute(query, params)\n                products = []\n                \n                for row in cursor.fetchall():\n                    products.append(ProductData(\n                        id=row[0],\n                        title=row[1],\n                        description=row[2],\n                        price=row[3],\n                        currency=row[4],\n                        availability=row[5],\n                        rating=row[6],\n                        reviews_count=row[7],\n                        images=json.loads(row[8]) if row[8] else [],\n                        categories=json.loads(row[9]) if row[9] else [],\n                        tags=json.loads(row[10]) if row[10] else [],\n                        seller=row[11],\n                        seller_url=row[12],\n                        upload_date=datetime.fromisoformat(row[13]) if row[13] else None,\n                        last_updated=datetime.fromisoformat(row[14]),\n                        url=row[15],\n                        source=row[16],\n                        specifications=json.loads(row[17]) if row[17] else {},\n                        shipping_info=json.loads(row[18]) if row[18] else {},\n                        variant_info=json.loads(row[19]) if row[19] else {},\n                        metadata=json.loads(row[20]) if row[20] else {}\n                    ))\n                \n                return products\n        except Exception as e:\n            logger.error(f"Failed to get products: {e}")\n            return []\n\nclass GmailManager:\n    """Gmail API manager for multiple accounts"""\n    \n    def __init__(self, config: GmailConfig):\n        self.config = config\n        self.current_credentials = None\n        self.service = None\n    \n    def authenticate(self, account_index: int = 0) -> bool:\n        """Authenticate with Gmail API"""\n        try:\n            if not self.config.accounts or account_index >= len(self.config.accounts):\n                logger.error("Invalid account index")\n                return False\n            \n            account = self.config.accounts[account_index]\n            \n            creds = None\n            if os.path.exists(self.config.token_file):\n                creds = Credentials.from_authorized_user_file(self.config.token_file)\n            \n            if not creds or not creds.valid:\n                if creds and creds.expired and creds.refresh_token:\n                    creds.refresh(Request())\n                else:\n                    flow = InstalledAppFlow.from_client_secrets_file(\n                        self.config.credentials_file,\n                        [\'https://www.googleapis.com/auth/gmail.send\']\n                    )\n                    creds = flow.run_local_server(port=0)\n                \n                with open(self.config.token_file, \'w\') as token:\n                    token.write(creds.to_json())\n            \n            self.current_credentials = creds\n            self.service = build(\'gmail\', \'v1\', credentials=creds)\n            self.config.current_account_index = account_index\n            \n            logger.info(f"Authenticated with Gmail account: {account[\'email\']}")\n            return True\n            \n        except Exception as e:\n            logger.error(f"Gmail authentication failed: {e}")\n            return False\n    \n    def send_email(self, to: str, subject: str, body: str, is_html: bool = False) -> bool:\n        """Send email via Gmail API"""\n        try:\n            if not self.service:\n                if not self.authenticate(self.config.current_account_index):\n                    return False\n            \n            message = MIMEMultipart()\n            message[\'to\'] = to\n            message[\'subject\'] = subject\n            \n            if is_html:\n                message.attach(MIMEText(body, \'html\'))\n            else:\n                message.attach(MIMEText(body, \'plain\'))\n            \n            raw_message = base64.urlsafe_b64encode(message.as_bytes()).decode()\n            \n            self.service.users().messages().send(\n                userId=\'me\',\n                body={\'raw\': raw_message}\n            ).execute()\n            \n            logger.info(f"Email sent successfully to {to}")\n            return True\n            \n        except Exception as e:\n            logger.error(f"Failed to send email: {e}")\n            return False\n    \n    def rotate_account(self) -> bool:\n        """Rotate to next Gmail account"""\n        next_index = (self.config.current_account_index + 1) % len(self.config.accounts)\n        return self.authenticate(next_index)\n\nclass ProductCategorizer:\n    """Automatic product categorization using keywords and patterns"""\n    \n    def __init__(self):\n        self.category_keywords = {\n            \'electronics\': [\n                \'phone\', \'laptop\', \'computer\', \'tablet\', \'tv\', \'camera\', \'headphone\',\n                \'speaker\', \'watch\', \'gadget\', \'smartphone\', \'notebook\', \'monitor\'\n            ],\n            \'clothing\': [\n                \'shirt\', \'pants\', \'dress\', \'jacket\', \'coat\', \'shoes\', \'boots\',\n                \'sneakers\', \'t-shirt\', \'jeans\', \'skirt\', \'blouse\', \'sweater\'\n            ],\n            \'home\': [\n                \'furniture\', \'sofa\', \'chair\', \'table\', \'bed\', \'mattress\', \'lamp\',\n                \'decor\', \'kitchen\', \'appliance\', \'vacuum\', \'cleaning\', \'storage\'\n            ],\n            \'beauty\': [\n                \'makeup\', \'cosmetic\', \'skincare\', \'cream\', \'lotion\', \'perfume\',\n                \'shampoo\', \'hair\', \'beauty\', \'face\', \'body\', \'nail\'\n            ],\n            \'sports\': [\n                \'fitness\', \'gym\', \'exercise\', \'sport\', \'outdoor\', \'bike\', \'running\',\n                \'yoga\', \'training\', \'equipment\', \'gear\', \'athletic\'\n            ],\n            \'books\': [\n                \'book\', \'novel\', \'textbook\', \'magazine\', \'comics\', \'ebook\',\n                \'reading\', \'literature\', \'fiction\', \'non-fiction\'\n            ],\n            \'toys\': [\n                \'toy\', \'game\', \'puzzle\', \'lego\', \'doll\', \'action figure\', \'board game\',\n                \'educational\', \'kids\', \'children\', \'play\'\n            ],\n            \'automotive\': [\n                \'car\', \'auto\', \'vehicle\', \'motorcycle\', \'parts\', \'accessories\',\n                \'tires\', \'engine\', \'battery\', \'oil\', \'maintenance\'\n            ]\n        }\n    \n    def categorize_product(self, title: str, description: str = "") -> List[str]:\n        """Categorize product based on title and description"""\n        text = f"{title} {description}".lower()\n        categories = []\n        \n        for category, keywords in self.category_keywords.items():\n            if any(keyword in text for keyword in keywords):\n                categories.append(category)\n        \n        # If no categories found, assign \'other\'\n        if not categories:\n            categories.append(\'other\')\n        \n        return categories\n    \n    def extract_tags(self, title: str, description: str = "") -> List[str]:\n        """Extract tags from product text"""\n        text = f"{title} {description}".lower()\n        \n        # Extract potential tags (single words, brands, etc.)\n        words = re.findall(r\'\\b\\w+\\b\', text)\n        \n        # Filter common stop words and short words\n        stop_words = {\'the\', \'and\', \'or\', \'but\', \'in\', \'on\', \'at\', \'to\', \'for\', \'of\', \'with\', \'by\'}\n        tags = [word for word in words if len(word) > 3 and word not in stop_words]\n        \n        # Remove duplicates and limit to 10 tags\n        tags = list(set(tags))[:10]\n        \n        return tags\n\nclass EnhancedProductScraper:\n    """Enhanced product scraper with all advanced features"""\n    \n    def __init__(self, config: ScraperConfig = None):\n        self.config = config or ScraperConfig()\n        self.status = ScraperStatus.IDLE\n        self.proxy_manager = ProxyManager(ProxyConfig())\n        self.db_manager = DatabaseManager()\n        self.gmail_manager = GmailManager(GmailConfig())\n        self.categorizer = ProductCategorizer()\n        self.session_stats = {\n            \'start_time\': None,\n            \'end_time\': None,\n            \'products_scraped\': 0,\n            \'errors_count\': 0,\n            \'current_session_id\': None\n        }\n        self.user_agents = [\n            \'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\',\n            \'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\',\n            \'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\'\n        ]\n    \n    async def scrape_product_page(self, url: str) -> Optional[ProductData]:\n        """Scrape single product page with enhanced features"""\n        try:\n            # Get proxy\n            proxy = self.proxy_manager.get_proxy()\n            \n            # Prepare headers\n            headers = {\n                \'User-Agent\': random.choice(self.user_agents),\n                \'Accept\': \'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\',\n                \'Accept-Language\': \'en-US,en;q=0.5\',\n                \'Accept-Encoding\': \'gzip, deflate\',\n                \'Connection\': \'keep-alive\',\n                \'Upgrade-Insecure-Requests\': \'1\',\n            }\n            \n            # Make request\n            async with aiohttp.ClientSession(\n                timeout=aiohttp.ClientTimeout(total=self.config.request_timeout),\n                headers=headers\n            ) as session:\n                async with session.get(url, proxy=proxy) as response:\n                    if response.status == 200:\n                        html = await response.text()\n                        return self.parse_product_page(html, url)\n                    else:\n                        logger.warning(f"HTTP {response.status} for {url}")\n                        if proxy:\n                            self.proxy_manager.mark_proxy_failed(proxy)\n                        return None\n                        \n        except Exception as e:\n            logger.error(f"Error scraping {url}: {e}")\n            self.session_stats[\'errors_count\'] += 1\n            return None\n    \n    def parse_product_page(self, html: str, url: str) -> Optional[ProductData]:\n        """Parse product page HTML with enhanced extraction"""\n        try:\n            soup = BeautifulSoup(html, \'html.parser\')\n            \n            # Extract basic information\n            title = self.extract_title(soup)\n            if not title:\n                return None\n            \n            price = self.extract_price(soup)\n            description = self.extract_description(soup)\n            availability = self.extract_availability(soup)\n            rating = self.extract_rating(soup)\n            reviews_count = self.extract_reviews_count(soup)\n            images = self.extract_images(soup, url)\n            seller = self.extract_seller(soup)\n            seller_url = self.extract_seller_url(soup)\n            upload_date = self.extract_upload_date(soup)\n            specifications = self.extract_specifications(soup)\n            shipping_info = self.extract_shipping_info(soup)\n            \n            # Generate unique ID\n            product_id = hashlib.md5(f"{title}{url}".encode()).hexdigest()\n            \n            # Auto-categorization\n            categories = []\n            tags = []\n            if self.config.enable_categorization:\n                categories = self.categorizer.categorize_product(title, description)\n                tags = self.categorizer.extract_tags(title, description)\n            \n            # Create product data\n            product = ProductData(\n                id=product_id,\n                title=title,\n                description=description,\n                price=price,\n                currency=\'USD\',  # Default, can be extracted\n                availability=availability,\n                rating=rating,\n                reviews_count=reviews_count,\n                images=images,\n                categories=categories,\n                tags=tags,\n                seller=seller,\n                seller_url=seller_url,\n                upload_date=upload_date,\n                last_updated=datetime.now(),\n                url=url,\n                source=urlparse(url).netloc,\n                specifications=specifications,\n                shipping_info=shipping_info,\n                variant_info={},  # Can be enhanced later\n                metadata={\n                    \'scraped_at\': datetime.now().isoformat(),\n                    \'scraper_version\': \'2.0\'\n                }\n            )\n            \n            return product\n            \n        except Exception as e:\n            logger.error(f"Error parsing product page: {e}")\n            return None\n    \n    def extract_title(self, soup: BeautifulSoup) -> Optional[str]:\n        """Extract product title"""\n        selectors = [\n            \'h1.product-title\',\n            \'h1[data-testid="product-title"]\',\n            \'h1.pdp-product-title\',\n            \'h1.product-name\',\n            \'h1.title\',\n            \'h1\',\n            \'[data-testid="heading"]\',\n            \'.product-title\',\n            \'.product-name\',\n            \'.title\'\n        ]\n        \n        for selector in selectors:\n            element = soup.select_one(selector)\n            if element:\n                title = element.get_text(strip=True)\n                if len(title) > 10:  # Reasonable title length\n                    return title\n        \n        return None\n    \n    def extract_price(self, soup: BeautifulSoup) -> float:\n        """Extract product price"""\n        price_selectors = [\n            \'.price\',\n            \'.current-price\',\n            \'.product-price\',\n            \'[data-testid="price"]\',\n            \'.price-current\',\n            \'.sale-price\',\n            \'.amount\'\n        ]\n        \n        for selector in price_selectors:\n            element = soup.select_one(selector)\n            if element:\n                price_text = element.get_text(strip=True)\n                price = self.parse_price(price_text)\n                if price > 0:\n                    return price\n        \n        return 0.0\n    \n    def parse_price(self, price_text: str) -> float:\n        """Parse price text to float"""\n        # Remove currency symbols and whitespace\n        price_text = re.sub(r\'[^\\d.,]\', \'\', price_text)\n        \n        # Handle different decimal separators\n        if \',\' in price_text and \'.\' in price_text:\n            # Assume format like 1,234.56\n            price_text = price_text.replace(\',\', \'\')\n        elif \',\' in price_text:\n            # Could be 1,234 or 1.234 (depending on locale)\n            parts = price_text.split(\',\')\n            if len(parts) == 2 and len(parts[1]) <= 2:\n                # Likely decimal separator (e.g., 123,45)\n                price_text = price_text.replace(\',\', \'.\')\n            else:\n                # Likely thousand separator\n                price_text = price_text.replace(\',\', \'\')\n        \n        try:\n            return float(price_text)\n        except ValueError:\n            return 0.0\n    \n    def extract_description(self, soup: BeautifulSoup) -> str:\n        """Extract product description"""\n        desc_selectors = [\n            \'.product-description\',\n            \'.description\',\n            \'[data-testid="product-description"]\',\n            \'.product-details\',\n            \'.details\',\n            \'.product-content\',\n            \'meta[name="description"]\'\n        ]\n        \n        for selector in desc_selectors:\n            if selector.startswith(\'meta\'):\n                element = soup.select_one(selector)\n                if element and element.get(\'content\'):\n                    return element.get(\'content\')\n            else:\n                element = soup.select_one(selector)\n                if element:\n                    desc = element.get_text(strip=True)\n                    if len(desc) > 20:\n                        return desc\n        \n        return ""\n    \n    def extract_availability(self, soup: BeautifulSoup) -> str:\n        """Extract product availability"""\n        availability_selectors = [\n            \'.availability\',\n            \'.stock-status\',\n            \'[data-testid="availability"]\',\n            \'.in-stock\',\n            \'.out-of-stock\'\n        ]\n        \n        for selector in availability_selectors:\n            element = soup.select_one(selector)\n            if element:\n                availability = element.get_text(strip=True).lower()\n                if \'out of stock\' in availability or \'unavailable\' in availability:\n                    return \'out_of_stock\'\n                elif \'in stock\' in availability or \'available\' in availability:\n                    return \'in_stock\'\n                elif \'limited\' in availability:\n                    return \'limited\'\n        \n        return \'unknown\'\n    \n    def extract_rating(self, soup: BeautifulSoup) -> Optional[float]:\n        """Extract product rating"""\n        rating_selectors = [\n            \'.rating\',\n            \'.stars\',\n            \'[data-testid="rating"]\',\n            \'.review-rating\',\n            \'.average-rating\'\n        ]\n        \n        for selector in rating_selectors:\n            element = soup.select_one(selector)\n            if element:\n                rating_text = element.get_text(strip=True)\n                rating = self.parse_rating(rating_text)\n                if rating is not None:\n                    return rating\n        \n        return None\n    \n    def parse_rating(self, rating_text: str) -> Optional[float]:\n        """Parse rating text to float"""\n        # Look for patterns like "4.5", "4.5 out of 5", "4.5/5"\n        patterns = [\n            r\'(\\d+\\.?\\d*)\\s*out\\s*of\\s*5\',\n            r\'(\\d+\\.?\\d*)/5\',\n            r\'(\\d+\\.?\\d*)\\s*stars?\',\n            r\'^(\\d+\\.?\\d*)$\'\n        ]\n        \n        for pattern in patterns:\n            match = re.search(pattern, rating_text, re.IGNORECASE)\n            if match:\n                try:\n                    rating = float(match.group(1))\n                    if 0 <= rating <= 5:\n                        return rating\n                except ValueError:\n                    continue\n        \n        return None\n    \n    def extract_reviews_count(self, soup: BeautifulSoup) -> Optional[int]:\n        """Extract reviews count"""\n        reviews_selectors = [\n            \'.reviews-count\',\n            \'.review-count\',\n            \'[data-testid="reviews-count"]\',\n            \'.num-reviews\',\n            \'.reviews\'\n        ]\n        \n        for selector in reviews_selectors:\n            element = soup.select_one(selector)\n            if element:\n                reviews_text = element.get_text(strip=True)\n                count = self.parse_number(reviews_text)\n                if count is not None:\n                    return count\n        \n        return None\n    \n    def parse_number(self, text: str) -> Optional[int]:\n        """Parse number from text"""\n        # Remove non-digit characters except for k, m, etc.\n        text = re.sub(r\'[^\\dkm.,]\', \'\', text.lower())\n        \n        # Handle k (thousands) and m (millions)\n        multiplier = 1\n        if \'k\' in text:\n            multiplier = 1000\n            text = text.replace(\'k\', \'\')\n        elif \'m\' in text:\n            multiplier = 1000000\n            text = text.replace(\'m\', \'\')\n        \n        text = text.replace(\',\', \'\')\n        \n        try:\n            return int(float(text) * multiplier)\n        except ValueError:\n            return None\n    \n    def extract_images(self, soup: BeautifulSoup, base_url: str) -> List[str]:\n        """Extract product images"""\n        images = []\n        \n        # Look for product images\n        img_selectors = [\n            \'.product-image img\',\n            \'.gallery img\',\n            \'.carousel img\',\n            \'[data-testid="product-image"]\',\n            \'.main-image img\',\n            \'.product-photo img\'\n        ]\n        \n        for selector in img_selectors:\n            for img in soup.select(selector):\n                src = img.get(\'src\') or img.get(\'data-src\')\n                if src:\n                    # Convert relative URLs to absolute\n                    if src.startswith(\'/\'):\n                        src = urljoin(base_url, src)\n                    elif src.startswith(\'//\'):\n                        src = \'https:\' + src\n                    \n                    if src not in images:\n                        images.append(src)\n        \n        return images[:5]  # Limit to 5 images\n    \n    def extract_seller(self, soup: BeautifulSoup) -> str:\n        """Extract seller information"""\n        seller_selectors = [\n            \'.seller-name\',\n            \'.brand\',\n            \'.vendor\',\n            \'[data-testid="seller"]\',\n            \'.store-name\'\n        ]\n        \n        for selector in seller_selectors:\n            element = soup.select_one(selector)\n            if element:\n                seller = element.get_text(strip=True)\n                if len(seller) > 2:\n                    return seller\n        \n        return ""\n    \n    def extract_seller_url(self, soup: BeautifulSoup) -> Optional[str]:\n        """Extract seller URL"""\n        seller_link_selectors = [\n            \'.seller-link a\',\n            \'.brand a\',\n            \'.vendor a\',\n            \'[data-testid="seller-link"]\'\n        ]\n        \n        for selector in seller_link_selectors:\n            element = soup.select_one(selector)\n            if element and element.get(\'href\'):\n                return element[\'href\']\n        \n        return None\n    \n    def extract_upload_date(self, soup: BeautifulSoup) -> Optional[datetime]:\n        """Extract product upload date"""\n        date_selectors = [\n            \'.upload-date\',\n            \'.listed-date\',\n            \'.published-date\',\n            \'[data-testid="upload-date"]\',\n            \'meta[property="article:published_time"]\',\n            \'meta[name="date"]\'\n        ]\n        \n        for selector in date_selectors:\n            element = soup.select_one(selector)\n            if element:\n                date_text = element.get(\'content\') or element.get_text(strip=True)\n                date = self.parse_date(date_text)\n                if date:\n                    return date\n        \n        return None\n    \n    def parse_date(self, date_text: str) -> Optional[datetime]:\n        """Parse date from text"""\n        # Common date formats\n        formats = [\n            \'%Y-%m-%d\',\n            \'%Y-%m-%dT%H:%M:%S\',\n            \'%Y-%m-%dT%H:%M:%SZ\',\n            \'%d/%m/%Y\',\n            \'%m/%d/%Y\',\n            \'%d-%m-%Y\',\n            \'%m-%d-%Y\'\n        ]\n        \n        for fmt in formats:\n            try:\n                return datetime.strptime(date_text.strip(), fmt)\n            except ValueError:\n                continue\n        \n        return None\n    \n    def extract_specifications(self, soup: BeautifulSoup) -> Dict[str, Any]:\n        """Extract product specifications"""\n        specs = {}\n        \n        # Look for specification tables or lists\n        spec_selectors = [\n            \'.specifications table\',\n            \'.specs table\',\n            \'.product-details table\',\n            \'.technical-specs table\',\n            \'.specs-list\'\n        ]\n        \n        for selector in spec_selectors:\n            element = soup.select_one(selector)\n            if element:\n                rows = element.select(\'tr\') or element.select(\'li\')\n                for row in rows:\n                    if row.name == \'tr\':\n                        cells = row.select(\'td, th\')\n                        if len(cells) >= 2:\n                            key = cells[0].get_text(strip=True)\n                            value = cells[1].get_text(strip=True)\n                            if key and value:\n                                specs[key] = value\n                    elif row.name == \'li\':\n                        text = row.get_text(strip=True)\n                        if \':\' in text:\n                            key, value = text.split(\':\', 1)\n                            specs[key.strip()] = value.strip()\n        \n        return specs\n    \n    def extract_shipping_info(self, soup: BeautifulSoup) -> Dict[str, Any]:\n        """Extract shipping information"""\n        shipping = {}\n        \n        shipping_selectors = [\n            \'.shipping-info\',\n            \'.delivery-info\',\n            \'.shipping-details\',\n            \'[data-testid="shipping"]\'\n        ]\n        \n        for selector in shipping_selectors:\n            element = soup.select_one(selector)\n            if element:\n                text = element.get_text(strip=True)\n                # Extract shipping cost, delivery time, etc.\n                if \'free shipping\' in text.lower():\n                    shipping[\'cost\'] = 0\n                    shipping[\'free\'] = True\n                \n                # Look for delivery time patterns\n                delivery_match = re.search(r\'delivery\\s*(?:in|within)\\s*(\\d+)\\s*(?:days?|business days?)\', text, re.IGNORECASE)\n                if delivery_match:\n                    shipping[\'delivery_days\'] = int(delivery_match.group(1))\n        \n        return shipping\n    \n    async def scrape_products(self, urls: List[str]) -> List[ProductData]:\n        """Scrape multiple product URLs with concurrent requests"""\n        self.status = ScraperStatus.RUNNING\n        self.session_stats[\'start_time\'] = datetime.now()\n        products = []\n        \n        logger.info(f"Starting to scrape {len(urls)} products")\n        \n        # Create semaphore to limit concurrent requests\n        semaphore = asyncio.Semaphore(self.config.concurrent_requests)\n        \n        async def scrape_with_semaphore(url: str) -> Optional[ProductData]:\n            async with semaphore:\n                await asyncio.sleep(self.config.delay_between_requests)\n                return await self.scrape_product_page(url)\n        \n        # Execute scraping tasks\n        tasks = [scrape_with_semaphore(url) for url in urls]\n        \n        for i, task in enumerate(asyncio.as_completed(tasks)):\n            try:\n                product = await task\n                if product:\n                    products.append(product)\n                    self.db_manager.save_product(product)\n                    self.session_stats[\'products_scraped\'] += 1\n                    \n                    # Limit products per run\n                    if len(products) >= self.config.max_products_per_run:\n                        break\n                \n                if i % 10 == 0:\n                    logger.info(f"Processed {i}/{len(urls)} URLs, found {len(products)} products")\n                    \n            except Exception as e:\n                logger.error(f"Error in scraping task: {e}")\n                self.session_stats[\'errors_count\'] += 1\n        \n        self.session_stats[\'end_time\'] = datetime.now()\n        self.status = ScraperStatus.COMPLETED\n        \n        logger.info(f"Scraping completed: {len(products)} products found, {self.session_stats[\'errors_count\']} errors")\n        \n        return products\n    \n    def export_to_csv(self, products: List[ProductData], filename: str = None) -> str:\n        """Export products to CSV file"""\n        if not filename:\n            filename = f"products_{datetime.now().strftime(\'%Y%m%d_%H%M%S\')}.csv"\n        \n        # Convert products to dictionary format\n        rows = []\n        for product in products:\n            row = asdict(product)\n            # Convert complex objects to JSON strings\n            row[\'images\'] = json.dumps(product.images)\n            row[\'categories\'] = json.dumps(product.categories)\n            row[\'tags\'] = json.dumps(product.tags)\n            row[\'specifications\'] = json.dumps(product.specifications)\n            row[\'shipping_info\'] = json.dumps(product.shipping_info)\n            row[\'variant_info\'] = json.dumps(product.variant_info)\n            row[\'metadata\'] = json.dumps(product.metadata)\n            rows.append(row)\n        \n        # Write to CSV\n        with open(filename, \'w\', newline=\'\', encoding=\'utf-8\') as csvfile:\n            if rows:\n                fieldnames = rows[0].keys()\n                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n                writer.writeheader()\n                writer.writerows(rows)\n        \n        logger.info(f"Exported {len(products)} products to {filename}")\n        return filename\n    \n    def send_to_admin_dashboard(self, products: List[ProductData]) -> bool:\n        """Send scraped products to admin dashboard"""\n        try:\n            # Convert products to JSON\n            products_data = [asdict(product) for product in products]\n            \n            # Send via email to admin\n            html_content = self.generate_admin_email_html(products_data)\n            \n            success = self.gmail_manager.send_email(\n                to="admin@jeantrail.com",  # Admin email\n                subject=f"New Products Scraped - {len(products)} items",\n                body=html_content,\n                is_html=True\n            )\n            \n            if success:\n                logger.info(f"Sent {len(products)} products to admin dashboard")\n                return True\n            else:\n                # Try next Gmail account\n                self.gmail_manager.rotate_account()\n                return self.gmail_manager.send_email(\n                    to="admin@jeantrail.com",\n                    subject=f"New Products Scraped - {len(products)} items",\n                    body=html_content,\n                    is_html=True\n                )\n                \n        except Exception as e:\n            logger.error(f"Failed to send products to admin dashboard: {e}")\n            return False\n    \n    def generate_admin_email_html(self, products_data: List[Dict]) -> str:\n        """Generate HTML email for admin dashboard"""\n        html = f"""\n        <!DOCTYPE html>\n        <html>\n        <head>\n            <title>New Products Report</title>\n            <style>\n                body {{ font-family: Arial, sans-serif; margin: 20px; }}\n                .header {{ background: #f0f0f0; padding: 20px; border-radius: 5px; }}\n                .product {{ border: 1px solid #ddd; margin: 10px 0; padding: 15px; border-radius: 5px; }}\n                .title {{ font-size: 18px; font-weight: bold; color: #333; }}\n                .price {{ font-size: 16px; color: #2ecc71; font-weight: bold; }}\n                .meta {{ font-size: 12px; color: #666; margin-top: 5px; }}\n                .stats {{ background: #e8f4f8; padding: 15px; border-radius: 5px; margin: 20px 0; }}\n            </style>\n        </head>\n        <body>\n            <div class="header">\n                <h1>üõçÔ∏è New Products Scraped Report</h1>\n                <p>Generated on: {datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\')}</p>\n            </div>\n            \n            <div class="stats">\n                <h2>üìä Statistics</h2>\n                <p><strong>Total Products:</strong> {len(products_data)}</p>\n                <p><strong>Scraping Duration:</strong> {self.session_stats[\'end_time\'] - self.session_stats[\'start_time\'] if self.session_stats[\'end_time\'] else \'N/A\'}</p>\n                <p><strong>Errors Encountered:</strong> {self.session_stats[\'errors_count\']}</p>\n            </div>\n            \n            <h2>üì¶ Products ({len(products_data)} items)</h2>\n        """\n        \n        for product in products_data[:20]:  # Limit to 20 products in email\n            html += f"""\n            <div class="product">\n                <div class="title">{product[\'title\']}</div>\n                <div class="price">${product[\'price\']:.2f} {product[\'currency\']}</div>\n                <div class="meta">\n                    <strong>Source:</strong> {product[\'source\']}<br>\n                    <strong>Category:</strong> {\', \'.join(json.loads(product[\'categories\']))}<br>\n                    <strong>Availability:</strong> {product[\'availability\']}<br>\n                    <strong>URL:</strong> <a href="{product[\'url\']}">{product[\'url\']}</a>\n                </div>\n            </div>\n            """\n        \n        if len(products_data) > 20:\n            html += f"<p><em>... and {len(products_data) - 20} more products</em></p>"\n        \n        html += """\n        </body>\n        </html>\n        """\n        \n        return html\n    \n    def get_status(self) -> Dict[str, Any]:\n        """Get current scraper status"""\n        return {\n            \'status\': self.status.value,\n            \'stats\': self.session_stats,\n            \'config\': asdict(self.config)\n        }\n\n# Example usage and main function\nasync def main():\n    """Main function demonstrating scraper usage"""\n    \n    # Initialize scraper with configuration\n    config = ScraperConfig(\n        max_products_per_run=50,\n        concurrent_requests=3,\n        delay_between_requests=2.0,\n        enable_categorization=True,\n        use_proxies=True,\n        enable_notifications=True\n    )\n    \n    scraper = EnhancedProductScraper(config)\n    \n    # Example URLs to scrape (replace with actual product URLs)\n    example_urls = [\n        "https://example-store.com/product1",\n        "https://example-store.com/product2",\n        "https://example-store.com/product3",\n    ]\n    \n    try:\n        # Scrape products\n        products = await scraper.scrape_products(example_urls)\n        \n        # Export to CSV\n        csv_file = scraper.export_to_csv(products)\n        print(f"Products exported to: {csv_file}")\n        \n        # Send to admin dashboard\n        if scraper.send_to_admin_dashboard(products):\n            print("Products sent to admin dashboard")\n        \n        # Print status\n        print(f"Final status: {scraper.get_status()}")\n        \n    except KeyboardInterrupt:\n        print("Scraping interrupted by user")\n    except Exception as e:\n        print(f"Error during scraping: {e}")\n        logger.error(f"Scraping error: {e}")\n\nif __name__ == "__main__":\n    # Run the main function\n    asyncio.run(main())\n</create-file>\n</function_calls>'}